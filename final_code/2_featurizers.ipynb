{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spe_vectorizers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spe_vectorizers.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "from SmilesPE.tokenizer import *\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def spe_featurizer(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses SPE vocabulary and CountVectorizer to create features\"\"\"\n",
    "    \n",
    "    # load vocab\n",
    "    spe_vob = codecs.open(r'SPE_ChEMBL.txt')\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "    \n",
    "    # split SMILES strings to tokens\n",
    "    train_spe = train_data.apply(lambda x: spe.tokenize(x))\n",
    "    test_spe = test_data.apply(lambda x: spe.tokenize(x))\n",
    "    \n",
    "    # split tokenized strings into tokens\n",
    "    # transform dataset into a matrix of vectors\n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = CountVectorizer(preprocessor=None, stop_words=None, lowercase=False, tokenizer=split_string)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_spe)\n",
    "    x_test = vectorizer.transform(test_spe)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab\n",
    "\n",
    "\n",
    "def spe_featurizer2(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses SPE vocabulary and CountVectorizer to create features\n",
    "       Forces the use of whole vocabulary, not just fragments in train data\"\"\"\n",
    "    \n",
    "    # load vocab\n",
    "    spe_vocab = pd.read_csv('SPE_ChEMBL.txt', header=None)\n",
    "    spe_vocab = spe_vocab.rename(columns={0: 'fragments'})\n",
    "    \n",
    "    spe_vob = codecs.open(r'SPE_ChEMBL.txt')\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "    \n",
    "    # split SMILES strings to tokens\n",
    "    train_spe = train_data.apply(lambda x: spe.tokenize(x))\n",
    "    test_spe = test_data.apply(lambda x: spe.tokenize(x))\n",
    "    \n",
    "    # split tokenized strings into tokens\n",
    "    # transform dataset into a matrix of vectors\n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = CountVectorizer(preprocessor=None, stop_words=None, lowercase=False, \n",
    "                                 tokenizer=split_string, vocabulary=spe_vocab.fragments)\n",
    "    x_train = vectorizer.transform(train_spe)\n",
    "    x_test = vectorizer.transform(test_spe)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab\n",
    "\n",
    "\n",
    "def atom_featurizer(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses atomwise tokenizer and CountVectorizer to create features\"\"\"\n",
    "    \n",
    "    # split SMILES strings into tokens\n",
    "    train_atom = train_data.apply(lambda x: ' '.join(atomwise_tokenizer(x)))\n",
    "    test_atom = test_data.apply(lambda x: ' '.join(atomwise_tokenizer(x)))\n",
    "    \n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = CountVectorizer(preprocessor=None, stop_words=None, \n",
    "                                 lowercase=False, tokenizer=split_string)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_atom)\n",
    "    x_test = vectorizer.transform(test_atom)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab\n",
    "\n",
    "\n",
    "def kmer_featurizer(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses atomwise tokenizer a\n",
    "       nd CountVectorizer to create features\"\"\"\n",
    "    \n",
    "    # split SMILES strings into tokens\n",
    "    train_kmer = train_data.apply(lambda x: ' '.join(kmer_tokenizer(x)))\n",
    "    test_kmer = test_data.apply(lambda x: ' '.join(kmer_tokenizer(x)))\n",
    "    \n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = CountVectorizer(preprocessor=None, stop_words=None, \n",
    "                                 lowercase=False, tokenizer=split_string)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_kmer)\n",
    "    x_test = vectorizer.transform(test_kmer)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spe_vectorizers_tfidf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spe_vectorizers_tfidf.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "from SmilesPE.tokenizer import *\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def spe_featurizer_tfidf(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses SPE vocabulary and CountVectorizer to create features\"\"\"\n",
    "    \n",
    "    # load vocab\n",
    "    spe_vob = codecs.open(r'SPE_ChEMBL.txt')\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "    \n",
    "    # split SMILES strings to tokens\n",
    "    train_spe = train_data.apply(lambda x: spe.tokenize(x))\n",
    "    test_spe = test_data.apply(lambda x: spe.tokenize(x))\n",
    "    \n",
    "    # split tokenized strings into tokens\n",
    "    # transform dataset into a matrix of vectors\n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = TfidfVectorizer(preprocessor=None, stop_words=None, lowercase=False, tokenizer=split_string)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_spe)\n",
    "    x_test = vectorizer.transform(test_spe)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab\n",
    "\n",
    "\n",
    "def spe_featurizer_tfidf2(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses SPE vocabulary and CountVectorizer to create features\n",
    "       Forces the use of whole vocabulary, not just fragments in train data\"\"\"\n",
    "    \n",
    "    # load vocab\n",
    "    spe_vocab = pd.read_csv('SPE_ChEMBL.txt', header=None)\n",
    "    spe_vocab = spe_vocab.rename(columns={0: 'fragments'})\n",
    "    \n",
    "    spe_vob = codecs.open(r'SPE_ChEMBL.txt')\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "    \n",
    "    # split SMILES strings to tokens\n",
    "    train_spe = train_data.apply(lambda x: spe.tokenize(x))\n",
    "    test_spe = test_data.apply(lambda x: spe.tokenize(x))\n",
    "    \n",
    "    # split tokenized strings into tokens\n",
    "    # transform dataset into a matrix of vectors\n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = TfidfVectorizer(preprocessor=None, stop_words=None, lowercase=False, \n",
    "                                 tokenizer=split_string, vocabulary=spe_vocab.fragments)\n",
    "    x_train = vectorizer.transform(train_spe)\n",
    "    x_test = vectorizer.transform(test_spe)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab\n",
    "\n",
    "\n",
    "def atom_featurizer_tfidf(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses atomwise tokenizer and CountVectorizer to create features\"\"\"\n",
    "    \n",
    "    # split SMILES strings into tokens\n",
    "    train_atom = train_data.apply(lambda x: ' '.join(atomwise_tokenizer(x)))\n",
    "    test_atom = test_data.apply(lambda x: ' '.join(atomwise_tokenizer(x)))\n",
    "    \n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = TfidfVectorizer(preprocessor=None, stop_words=None, \n",
    "                                 lowercase=False, tokenizer=split_string)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_atom)\n",
    "    x_test = vectorizer.transform(test_atom)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab\n",
    "\n",
    "\n",
    "def kmer_featurizer_tfidf(train_data, test_data):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses atomwise tokenizer a\n",
    "       nd CountVectorizer to create features\"\"\"\n",
    "    \n",
    "    # split SMILES strings into tokens\n",
    "    train_kmer = train_data.apply(lambda x: ' '.join(kmer_tokenizer(x)))\n",
    "    test_kmer = test_data.apply(lambda x: ' '.join(kmer_tokenizer(x)))\n",
    "    \n",
    "    split_string = lambda x: x.split()\n",
    "    vectorizer = TfidfVectorizer(preprocessor=None, stop_words=None, \n",
    "                                 lowercase=False, tokenizer=split_string)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(train_kmer)\n",
    "    x_test = vectorizer.transform(test_kmer)\n",
    "    train_vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_test, train_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kaggle_chem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kaggle_chem.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import word2vec\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "\n",
    "def ecfp_featurizer(train, test):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses mol2vec to create ECFP features from SMILES strings\"\"\"\n",
    "    \n",
    "    # convert SMILES to RDKit Mol object\n",
    "    train['mol'] = train['std_compounds'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    test['mol'] = test['std_compounds'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    \n",
    "    model = word2vec.Word2Vec.load('model_300dim.pkl')\n",
    "    \n",
    "    #Constructing sentences\n",
    "    train['sentence'] = train.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    test['sentence'] = test.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "\n",
    "    # extracting embeddings to a numpy.array\n",
    "    # note that we always should mark unseen='UNK' in sentence2vec() \n",
    "    # so that model is taught how to handle unknown substructures\n",
    "    train['mol2vec'] = [DfVec(x) for x in sentences2vec(train['sentence'], model, unseen='UNK')]\n",
    "    test['mol2vec'] = [DfVec(x) for x in sentences2vec(test['sentence'], model, unseen='UNK')]\n",
    "    x_train = np.array([x.vec for x in train['mol2vec']])\n",
    "    x_test = np.array([x.vec for x in test['mol2vec']])\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def number_of_atoms(atom_list, df):\n",
    "    \"\"\"Helper function for oned_featurizer\"\"\"\n",
    "    \n",
    "    for i in atom_list:\n",
    "        df['num_of_{}_atoms'.format(i)] = df['mol'].apply(lambda x: len(x.GetSubstructMatches(Chem.MolFromSmiles(i))))\n",
    "        \n",
    "def oned_featurizer(train, test):\n",
    "    \"\"\"Creates datasets ready to input into ML models\n",
    "       Uses mol2vec to create 1D representations of molecules from SMILES strings\n",
    "       Includes the following features: number of atoms, number of heavy atoms,\n",
    "           number of C, O, N, and Cl atoms, molecular weight, \n",
    "           number of valence electrons, and number of heteroatoms\"\"\"\n",
    "    \n",
    "    # convert SMILES to RDKit Mol object\n",
    "    train['mol'] = train['std_compounds'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    test['mol'] = test['std_compounds'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    \n",
    "    # number of atoms\n",
    "    train['mol'] = train['mol'].apply(lambda x: Chem.AddHs(x))\n",
    "    train['num_of_atoms'] = train['mol'].apply(lambda x: x.GetNumAtoms())\n",
    "    train['num_of_heavy_atoms'] = train['mol'].apply(lambda x: x.GetNumHeavyAtoms())\n",
    "    number_of_atoms(['C','O', 'N', 'Cl'], train)\n",
    "    \n",
    "    test['mol'] = test['mol'].apply(lambda x: Chem.AddHs(x))\n",
    "    test['num_of_atoms'] = test['mol'].apply(lambda x: x.GetNumAtoms())\n",
    "    test['num_of_heavy_atoms'] = train['mol'].apply(lambda x: x.GetNumHeavyAtoms())\n",
    "    number_of_atoms(['C','O', 'N', 'Cl'], test)\n",
    "    \n",
    "    # molecular descriptors\n",
    "    train['mol_w'] = train['mol'].apply(lambda x: Descriptors.ExactMolWt(x))\n",
    "    train['num_valence_electrons'] = train['mol'].apply(lambda x: Descriptors.NumValenceElectrons(x))\n",
    "    train['num_heteroatoms'] = train['mol'].apply(lambda x: Descriptors.NumHeteroatoms(x))\n",
    "    \n",
    "    test['mol_w'] = test['mol'].apply(lambda x: Descriptors.ExactMolWt(x))\n",
    "    test['num_valence_electrons'] = test['mol'].apply(lambda x: Descriptors.NumValenceElectrons(x))\n",
    "    test['num_heteroatoms'] = test['mol'].apply(lambda x: Descriptors.NumHeteroatoms(x))\n",
    "    \n",
    "    x_train = train[['num_of_atoms', 'num_of_heavy_atoms', 'num_of_C_atoms', \n",
    "                     'num_of_O_atoms', 'num_of_N_atoms', 'num_of_Cl_atoms', 'mol_w', \n",
    "                     'num_valence_electrons', 'num_heteroatoms']]\n",
    "    x_test = test[['num_of_atoms', 'num_of_heavy_atoms', 'num_of_C_atoms', \n",
    "                     'num_of_O_atoms', 'num_of_N_atoms', 'num_of_Cl_atoms', 'mol_w', \n",
    "                     'num_valence_electrons', 'num_heteroatoms']]\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
